# FAQ

## ü™≤ How to debug the problems with the system?

```console
kubectl logs -f -n keycloak statefulset.apps/keycloak
```

---

## Why is deployed as StatefulSet?

Keycloak pods are more or less stateless. They are clustered using Infinispan under the hoods, to be as much scalable
as desired. The key here is that not all the information is stored distributed, but some data, are stored locally in the pods,
for example, session data, which means if you start the auth flow with a node, only that node stores the data.

This mechanism is strong, because if some node does not know some data, simply asks other nodes for it, reducing the
performance. Do you imagine a lot of nodes spamming others for the session data?

Keycloak includes a mechanism to distribute some data that are defaulted to be local, so we chose to distribute the sessions
too. And the number of nodes which store those data has to be set manually inside a `ConfigMap`, so we decided to launch
the system as a `StatefulSet` to make others understand that there is a cluster under the hoods, with minimal manual
intervention, but not auto-managed.

Variables involved are `CACHE_OWNERS_COUNT: "3"` and `CACHE_OWNERS_AUTH_SESSIONS_COUNT: "3"`

---

## Why the master realm is empty?

It is used for management purposes of other realms. This is a good practise established by RedHat in the Keycloak
official documentation. We just accommodate to it.

---

## üò® I destroyed the system. What do I do?

While deploying this system, we have tried several operators to maintain PostgreSQL with the minimal effort. We wanted
to keep this easy to maintain because using a database was not a final goal, but an intermediate one to maintain this system,
which is the final one.

To make this reliable, we needed a good disaster recovery strategy, and most operators out there use `pgrest` to store
and restore backups. The bad point using ths approach was random failures when restoring. And this happened for most them.

We changed the approach to do disaster recovery to use the recently graduated VolumeSnapshot API inside Kubernetes, but
needed predictable names for the PVCs and PVs generated by the operator. Remember you need to turn off everything,
generate the PVCs from the VolumeSnapshots and then turn on the database. So it needs to recognize the PVCs to work.

After testing a lot of operators for this, finally we decided to use Zalando's operator due to predictable names on
generated PVCs on creation time.

In this repository, we won't cover the disaster recovery strategy, due to it is out of the scope. Moreover, each
company prefers to craft its own strategy, and some of them out there will prefer to rely on `pgrest` already integrated
on most of the Postgres operators.

For those reasons, it's better to guide you to the official
[Zalando's Postgres Operator](https://postgres-operator.readthedocs.io/en/latest/administrator/#wal-archiving-and-physical-basebackups)

---

Cheers! üçªüçªüçª
